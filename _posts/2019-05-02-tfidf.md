---
layout: default
title: 什么是TF-IDF
categories: NLP
tags: Newbie, tfidf
---
# TF-IDF

### TF-IDF

##### Term Frequency-Inverse Document Frequency (词频-逆文件频率)

是一种用于资讯检索与文本挖掘的常用加权技术，是一种统计方法，用于评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。

直观来说，字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

### TF

##### Term Frequency (词频)

指的是某一个给定的词语在该文件中出现的次数，这个数字通常会被归一化（一般是词频除以文章总词数），以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，与重要性无关）

$TF_w = \frac{在某一类中词条w出现的次数}{该类中所有的词条数目}$

###### Note: 一些通用词语对于主题并没有太大的作用，所以仅仅使用TF不合适。权重设计：一个词预测主题的能力越强，权重越大，反之亦然。所有统计的文章中，一些词只是在其中很少几篇文章中出现，那么这样的词对文章主题的作用更大，权重更大。IDF完成的是这样的工作。



### IDF

##### Inverse Document Frequency (逆向文件频率)

如果包含词条t的文档越少，IDF越大，说明词条具有很好的类别区分能力。某一特定词语的IDF，由总文件数除以包含该词语的文件数目，再取对数得到。

$IDF = log(\frac{语料库的文档总数}{包含词条w的文档数+1})$

###### Note: 分母+1是为了避免分母为0



某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生高权重的TF-IDF. 因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。

$TF-IDF = TF * IDF$  



### Pros and Cons

##### Pros:简单快速，容易理解

##### Cons: 

##### 1. 用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多 

##### 2. 这种计算无法体现位置信息，无法体现词在上下文的重要性 （如果要体现词的上下文结构，你可能需要使用word2vec）



## Reference

[【Wiki】 tf-idf](https://zh.wikipedia.org/wiki/Tf-idf)

[【CSDN】 TF-IDF原理及使用]( https://blog.csdn.net/zrc199021/article/details/53728499)

[【知乎】机器学习: 生动理解TF-IDF算法](https://zhuanlan.zhihu.com/p/31197209)

<a href="{{ site.baseurl }}/index.html">Home</a>



